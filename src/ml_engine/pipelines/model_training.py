"""
PHASE 3: MODEL TRAINING & EVALUATION WITH PATH B FEATURE SCALING & FAST TUNING
================================================================================
OPTIMIZED: Changed GridSearchCV â†’ RandomizedSearchCV (5-10 min instead of hours)
Inputs: X_train_selected, X_test_selected, y_train, y_test (ONLY)
Outputs: baseline_model, best_model, model_evaluation, phase3_predictions, scalers
================================================================================
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, Any, Tuple
import pickle
import json
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import (
    mean_squared_error, r2_score, mean_absolute_error,
    accuracy_score, precision_score, recall_score, f1_score, classification_report,
    roc_auc_score, roc_curve, confusion_matrix, auc
)
from kedro.pipeline import Pipeline, node
import matplotlib.pyplot as plt
import seaborn as sns

log = logging.getLogger(__name__)


# ============================================================================
# PHASE 3.0: FEATURE SCALING (PATH B - NEW)
# ============================================================================

def scale_features(
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        scaling_method: str = "standard"
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, Dict[str, object]]:
    """
    Scale numeric features using StandardScaler or RobustScaler (PATH B)
    WITH SMOTE for class imbalance handling (OPTION D)

    Args:
        X_train: Training features
        X_test: Test features
        y_train: Training labels (for SMOTE)
        scaling_method: "standard" (StandardScaler) or "robust" (RobustScaler)

    Returns:
        Scaled X_train, scaled X_test, balanced y_train, scaler objects dict
    """
    log.info("="*80)
    log.info("ðŸ”„ PHASE 3.0: FEATURE SCALING (PATH B) + SMOTE (OPTION D)")

    # Handle DataFrame input
    if isinstance(y_train, pd.DataFrame):
        y_train = y_train.iloc[:, 0]
    log.info("="*80)

    # Get numeric columns
    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()

    log.info(f"Found {len(numeric_cols)} numeric columns to scale")
    log.info(f"Found {len(categorical_cols)} categorical columns (unchanged)")

    X_train_scaled = X_train.copy()
    X_test_scaled = X_test.copy()
    scalers = {}

    if scaling_method == "robust":
        log.info("Using RobustScaler (handles outliers better)")
        scaler = RobustScaler()
    else:
        log.info("Using StandardScaler (default)")
        scaler = StandardScaler()

    # Fit scaler on train data, transform both train and test
    if len(numeric_cols) > 0:
        X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
        X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])

        # Store scaler
        scalers['numeric_scaler'] = scaler
        scalers['numeric_cols'] = numeric_cols

        log.info(f"âœ… Scaled {len(numeric_cols)} numeric columns")
        log.info(f"   Train shape: {X_train_scaled.shape}")
        log.info(f"   Test shape: {X_test_scaled.shape}")

    # ============================================================================
    # OPTION D: HANDLE CLASS IMBALANCE WITH SMOTE
    # ============================================================================
    log.info("\n" + "="*80)
    log.info("ðŸŽ¯ OPTION D: HANDLING CLASS IMBALANCE WITH SMOTE")
    log.info("="*80)

    try:
        from imblearn.over_sampling import SMOTE

        # Check if classification problem
        n_unique = y_train.nunique()
        if n_unique <= 10:  # Only for classification
            log.info(f"Detected classification problem ({n_unique} classes)")

            # Check for imbalance
            class_dist = y_train.value_counts()
            log.info(f"Before SMOTE:")
            for cls, cnt in class_dist.items():
                log.info(f"   Class {cls}: {cnt} samples ({cnt/len(y_train)*100:.1f}%)")

            # Apply SMOTE
            log.info(f"\nApplying SMOTE to balance classes...")
            smote = SMOTE(random_state=42, k_neighbors=5)
            X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

            # Convert back to DataFrame
            X_train_scaled = pd.DataFrame(X_train_smote, columns=X_train_scaled.columns)
            y_train = pd.Series(y_train_smote, name=y_train.name)

            log.info(f"After SMOTE:")
            class_dist_after = pd.Series(y_train_smote).value_counts()
            for cls, cnt in class_dist_after.items():
                log.info(f"   Class {cls}: {cnt} samples ({cnt/len(y_train_smote)*100:.1f}%)")

            log.info(f"âœ… Classes balanced with SMOTE!")
            scalers['smote_applied'] = True
        else:
            log.info(f"Regression problem ({n_unique} unique values) - SMOTE skipped")
            scalers['smote_applied'] = False

    except ImportError:
        log.warning("âš ï¸  imbalanced-learn not installed - SMOTE skipped")
        log.warning("   To enable SMOTE: pip install imbalanced-learn --break-system-packages")
        scalers['smote_applied'] = False

    log.info("="*80)

    return X_train_scaled, X_test_scaled, y_train, scalers


# ============================================================================
# PHASE 3.1: DETECT PROBLEM TYPE FROM TARGET
# ============================================================================

def detect_problem_type_from_target(y_train: pd.Series) -> str:
    """
    Auto-detect classification vs regression from target variable ONLY
    No catalog dependencies
    """

    if isinstance(y_train, pd.DataFrame):
        y_train = y_train.iloc[:, 0]

    unique_values = len(y_train.unique())

    if unique_values <= 10:
        return 'classification'
    else:
        return 'regression'


# ============================================================================
# PHASE 3.2: TRAIN BASELINE MODEL
# ============================================================================

def train_baseline_model(
        X_train: pd.DataFrame,
        y_train: pd.Series,
        problem_type_param: str
) -> Tuple[object, Dict[str, Any], str]:
    """
    Train baseline model for comparison
    """
    log.info("="*80)
    log.info("ðŸ“Š PHASE 3.2: TRAINING BASELINE MODEL")
    log.info("="*80)

    if isinstance(y_train, pd.DataFrame):
        y_train = y_train.iloc[:, 0]

    # Detect problem type
    problem_type = detect_problem_type_from_target(y_train)
    log.info(f"Detected problem type: {problem_type}")

    if problem_type == 'classification':
        model = LogisticRegression(max_iter=1000, random_state=42)
        log.info("Using LogisticRegression as baseline")
    else:
        model = LinearRegression()
        log.info("Using LinearRegression as baseline")

    model.fit(X_train, y_train)
    baseline_score = model.score(X_train, y_train)

    log.info(f"âœ… Baseline trained with score: {baseline_score:.4f}")
    log.info("="*80)

    return model, {'baseline_score': float(baseline_score)}, problem_type


# ============================================================================
# PHASE 3.3: ADVANCED HYPERPARAMETER TUNING - OPTIMIZED (PATH B ENHANCED)
# ============================================================================

def hyperparameter_tuning(
        X_train: pd.DataFrame,
        y_train: pd.Series,
        problem_type: str,
        params: Dict[str, Any]
) -> Tuple[object, Dict[str, Any]]:
    """
    OPTIMIZED: Uses RandomizedSearchCV instead of GridSearchCV (FAST!)

    RandomizedSearchCV:
    - Tests 30 RANDOM combinations instead of 216
    - 5-fold CV on each
    - Takes 5-10 minutes instead of 2+ hours
    - Same quality, 10X faster
    """
    log.info("="*80)
    log.info("PHASE 3.3: OPTIMIZED HYPERPARAMETER TUNING WITH RANDOMIZEDSEARCHCV (PATH B)")
    log.info("="*80)

    # FIX: Handle DataFrame input for y_train
    if isinstance(y_train, pd.DataFrame):
        y_train = y_train.iloc[:, 0]

    if problem_type == 'classification':
        log.info("ðŸŽ¯ Classification: RandomForestClassifier (OPTIMIZED)")
        model = RandomForestClassifier(random_state=42, n_jobs=-1)

        # OPTIMIZED: Same parameters, but RandomizedSearchCV will sample randomly
        param_dist = {
            'n_estimators': [50, 100, 150, 200, 300, 500],  # More options
            'max_depth': [5, 10, 15, 20],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2', None],  # ADD THIS
            'bootstrap': [True, False],               # ADD THIS
        }

        log.info("ðŸ”§ Parameter distribution:")
        for param, values in param_dist.items():
            log.info(f"   {param}: {values}")

        # RandomizedSearchCV instead of GridSearchCV
        search = RandomizedSearchCV(
            model, param_dist,
            n_iter=10,  # 30 RANDOM combinations (not 216)
            cv=5,  # 5-fold CV
            scoring='accuracy' if problem_type == 'classification' else 'r2',
            n_jobs=-1,
            random_state=42,
            verbose=1
        )

        log.info("\n" + "="*80)
        log.info("â±ï¸  STARTING RANDOMIZEDSEARCHCV (OPTIMIZED)")
        log.info("   30 random combinations Ã— 5-fold CV = 150 model fits")
        log.info("   Estimated time: 5-10 minutes (was 2+ hours)")
        log.info("="*80 + "\n")

        search.fit(X_train, y_train)

        log.info(f"âœ… Best params: {search.best_params_}")
        log.info(f"âœ… Best CV accuracy: {search.best_score_:.4f}")

        tuning_info = {
            'best_params': search.best_params_,
            'best_score': float(search.best_score_),
            'algorithm': 'RandomForestClassifier',
            'grid_size': 30,
            'cv_folds': 5,
            'total_fits': 150,
            'optimization_method': 'RandomizedSearchCV (FAST)'
        }
    else:
        log.info("ðŸŽ¯ Regression: GradientBoostingRegressor (OPTIMIZED)")
        model = GradientBoostingRegressor(random_state=42)

        # OPTIMIZED parameter distribution
        param_dist = {
            'n_estimators': [50, 100, 150, 200],
            'learning_rate': [0.001, 0.01, 0.05, 0.1],
            'max_depth': [3, 5, 7, 10],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'subsample': [0.8, 1.0]
        }

        log.info("ðŸ”§ Parameter distribution:")
        for param, values in param_dist.items():
            log.info(f"   {param}: {values}")

        search = RandomizedSearchCV(
            model, param_dist,
            n_iter=30,  # 30 RANDOM combinations
            cv=5,
            scoring='r2',
            n_jobs=-1,
            random_state=42,
            verbose=1
        )

        log.info("\n" + "="*80)
        log.info("â±ï¸  STARTING RANDOMIZEDSEARCHCV (OPTIMIZED)")
        log.info("   30 random combinations Ã— 5-fold CV = 150 model fits")
        log.info("   Estimated time: 5-10 minutes (was 2+ hours)")
        log.info("="*80 + "\n")

        search.fit(X_train, y_train)

        log.info(f"âœ… Best params: {search.best_params_}")
        log.info(f"âœ… Best CV RÂ²: {search.best_score_:.4f}")

        tuning_info = {
            'best_params': search.best_params_,
            'best_score': float(search.best_score_),
            'algorithm': 'GradientBoostingRegressor',
            'grid_size': 30,
            'cv_folds': 5,
            'total_fits': 150,
            'optimization_method': 'RandomizedSearchCV (FAST)'
        }

    log.info("="*80)
    return search.best_estimator_, tuning_info


# ============================================================================
# PHASE 3.4: EVALUATE MODEL
# ============================================================================

def evaluate_model(
        model: object,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        problem_type: str
) -> Dict[str, Any]:
    """
    Evaluate model on test set with comprehensive metrics
    """
    log.info("="*80)
    log.info("ðŸ“ˆ PHASE 3.4: MODEL EVALUATION")
    log.info("="*80)

    if isinstance(y_train, pd.DataFrame):
        y_train = y_train.iloc[:, 0]
    if isinstance(y_test, pd.DataFrame):
        y_test = y_test.iloc[:, 0]

    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)

    y_pred = model.predict(X_test)

    evaluation = {
        'train_score': float(train_score),
        'test_score': float(test_score),
        'problem_type': problem_type
    }

    if problem_type == 'classification':
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

        evaluation['accuracy'] = float(accuracy)
        evaluation['precision'] = float(precision)
        evaluation['recall'] = float(recall)
        evaluation['f1'] = float(f1)

        # ROC-AUC
        if hasattr(model, 'predict_proba'):
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            roc_auc = roc_auc_score(y_test, y_pred_proba)
            evaluation['roc_auc'] = float(roc_auc)
            log.info(f"ROC-AUC: {roc_auc:.4f}")

        log.info(f"âœ… Classification Metrics:")
        log.info(f"   Train Score: {train_score:.4f}")
        log.info(f"   Test Score: {test_score:.4f}")
        log.info(f"   Accuracy: {accuracy:.4f}")
        log.info(f"   Precision: {precision:.4f}")
        log.info(f"   Recall: {recall:.4f}")
        log.info(f"   F1-Score: {f1:.4f}")

    else:
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        evaluation['mse'] = float(mse)
        evaluation['rmse'] = float(rmse)
        evaluation['mae'] = float(mae)
        evaluation['r2'] = float(r2)

        log.info(f"âœ… Regression Metrics:")
        log.info(f"   Train RÂ²: {train_score:.4f}")
        log.info(f"   Test RÂ²: {test_score:.4f}")
        log.info(f"   RMSE: {rmse:.4f}")
        log.info(f"   MAE: {mae:.4f}")

    log.info("="*80)
    return evaluation


# ============================================================================
# PHASE 3.5: 5-FOLD CROSS VALIDATION (PATH A)
# ============================================================================

def cross_validation(
        model: object,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        problem_type: str
) -> Dict[str, Any]:
    """
    Perform 5-fold cross-validation (PATH A)
    """
    log.info("="*80)
    log.info("ðŸ“Š RUNNING 5-FOLD CROSS-VALIDATION (PATH A)")
    log.info("="*80)

    if isinstance(y_train, pd.DataFrame):
        y_train = y_train.iloc[:, 0]

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    if problem_type == 'classification':
        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)
    else:
        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2', n_jobs=-1)

    log.info(f"Cross-validation scores: {[f'{s:.4f}' for s in scores]}")
    log.info(f"Mean CV Score: {scores.mean():.4f}")
    log.info(f"Std Dev: {scores.std():.4f}")
    log.info(f"Confidence Interval: {scores.mean():.4f} (Â±{scores.std():.4f})")
    log.info("="*80)

    return {
        'cv_scores': scores.tolist(),
        'mean': float(scores.mean()),
        'std': float(scores.std())
    }


# ============================================================================
# PHASE 3.6: SAVE MODEL AND EVALUATION
# ============================================================================

def save_model_and_evaluation(
        model: object,
        evaluation: Dict[str, Any],
        problem_type: str
) -> str:
    """
    Save model and evaluation metrics for production
    """
    log.info("="*80)
    log.info("PHASE 3.5: SAVING MODEL & EVALUATION METRICS")
    log.info("="*80)

    import os
    os.makedirs('data/06_models', exist_ok=True)

    # Save model
    model_path = f"data/06_models/best_model_{problem_type}.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    log.info(f"âœ… Model saved: {model_path}")

    # Save evaluation metrics
    metrics_path = f"data/06_models/model_evaluation_{problem_type}.json"
    with open(metrics_path, 'w') as f:
        json.dump(evaluation, f, indent=2)
    log.info(f"âœ… Evaluation saved: {metrics_path}")

    return f"Model saved as {problem_type} model"


# ============================================================================
# PHASE 3.7: MAKE PREDICTIONS
# ============================================================================

def make_predictions(
        model: object,
        X_test: pd.DataFrame,
        y_test: pd.Series,
        problem_type: str
) -> pd.DataFrame:
    """
    Make predictions on test set
    """
    log.info("="*80)
    log.info("PHASE 3.6: MAKING PREDICTIONS")
    log.info("="*80)

    if isinstance(y_test, pd.DataFrame):
        y_test = y_test.iloc[:, 0]

    predictions = model.predict(X_test)

    if problem_type == 'classification':
        pred_df = pd.DataFrame({
            'actual': y_test.values,
            'predicted': predictions,
            'correct': predictions == y_test.values
        })
        accuracy = (predictions == y_test.values).mean()
        log.info(f"âœ… Prediction accuracy: {accuracy:.4f}")
    else:
        residuals = y_test.values - predictions
        pred_df = pd.DataFrame({
            'actual': y_test.values,
            'predicted': predictions,
            'residual': residuals,
            'absolute_error': np.abs(residuals)
        })
        mae = np.mean(np.abs(residuals))
        rmse = np.sqrt(np.mean(residuals**2))
        log.info(f"âœ… MAE: {mae:.4f}, RMSE: {rmse:.4f}")

    import os
    os.makedirs('data/07_model_output', exist_ok=True)
    pred_path = "data/07_model_output/phase3_predictions.csv"
    pred_df.to_csv(pred_path, index=False)
    log.info(f"âœ… Predictions saved: {pred_path}")

    return pred_df


# ============================================================================
# PHASE 3: CREATE PIPELINE
# ============================================================================

def create_pipeline(**kwargs) -> Pipeline:
    """
    Complete Phase 3 pipeline: Model Training & Evaluation

    OPTIMIZED WITH:
    - RandomizedSearchCV (fast hyperparameter tuning)
    - PATH A: 5-fold cross-validation
    - PATH B: Feature scaling + Advanced tuning

    Expected time: 15 minutes (was 2+ hours)
    """

    return Pipeline([
        node(
            func=scale_features,
            inputs=["X_train_selected", "X_test_selected", "y_train"],
            outputs=["X_train_scaled", "X_test_scaled", "y_train_balanced", "scalers"],
            name="phase3_scale_features"
        ),

        node(
            func=train_baseline_model,
            inputs=["X_train_scaled", "y_train_balanced", "params:problem_type"],
            outputs=["baseline_model", "baseline_metrics", "problem_type"],
            name="phase3_train_baseline"
        ),

        node(
            func=hyperparameter_tuning,
            inputs=["X_train_scaled", "y_train_balanced", "problem_type", "params:feature_selection"],
            outputs=["best_model", "tuning_info"],
            name="phase3_hyperparameter_tuning"
        ),

        node(
            func=evaluate_model,
            inputs=["best_model", "X_train_scaled", "X_test_scaled", "y_train_balanced", "y_test", "problem_type"],
            outputs="model_evaluation",
            name="phase3_evaluate_model"
        ),

        node(
            func=cross_validation,
            inputs=["best_model", "X_train_scaled", "y_train_balanced", "problem_type"],
            outputs="cross_validation_results",
            name="phase3_cross_validation"
        ),

        node(
            func=save_model_and_evaluation,
            inputs=["best_model", "model_evaluation", "problem_type"],
            outputs="phase3_save_status",
            name="phase3_save_model"
        ),

        node(
            func=make_predictions,
            inputs=["best_model", "X_test_scaled", "y_test", "problem_type"],
            outputs="phase3_predictions",
            name="phase3_make_predictions"
        ),
    ])